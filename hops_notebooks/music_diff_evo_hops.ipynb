{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Music NN\n",
    "This model requires to add a .zip file with the song batches in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7303</td><td>application_1513605045578_4842</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_4842/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop7:8042/node/containerlogs/container_e28_1513605045578_4842_01_000001/demo_tensorflow_marccr01__marccr00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper_music(filters = 128, filters_end=128,window=5,window_end=5, learning_rate = 0.01, dropout = 0.7, epochs = 10, batch_size = 100):\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Input, GlobalMaxPooling1D, Dropout\n",
    "    from keras.utils import normalize\n",
    "    from keras import optimizers\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import keras\n",
    "    import random\n",
    "    from hops import hdfs\n",
    "    import time\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "    start = time.time()\n",
    "    print(\"Start time: \" + str(start))\n",
    "\n",
    "\n",
    "    def createModel(filters,filters_end,window,window_end, learning_rate, dropout):\n",
    "        '''Creates the NN model. As an input, the NN expects a normalized batch of 10 132300-dimensional\n",
    "        For now, the following parameters are added to the model.\n",
    "        @:param filters: number of filters in all the conv1d layers {50, 300}\n",
    "        @:param learning_rate: learning rate of the model {0.0, 1.0}\n",
    "        @:param dropout: Dropout value {0.0, 1.0}\n",
    "            '''\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(MaxPooling1D(1,input_shape = (132300,1)))\n",
    "\n",
    "        model.add(Conv1D(100, 100, strides=10 ,activation='sigmoid'))\n",
    "    \n",
    "        model.add(Conv1D(50, 100, strides=5 ,activation='sigmoid'))\n",
    "\n",
    "        model.add(Conv1D(20, 100, strides=5 ,activation='sigmoid'))\n",
    "\n",
    "\n",
    "        model.add(Conv1D(20, 20, activation='relu'))\n",
    "        model.add(MaxPooling1D(5))\n",
    "\n",
    "        model.add(Conv1D(10, 2, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        print(str(model.summary()))\n",
    "\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train_test_split(all_files, test_size=0.3):\n",
    "        '''\n",
    "        Randomly splits files into train and test files\n",
    "        '''\n",
    "        lenfiles = len(all_files)\n",
    "        lentest = int(lenfiles * test_size)\n",
    "        lentrain = lenfiles - lentest\n",
    "\n",
    "        test = random.sample(all_files, lentest)\n",
    "        train = list(set(all_files) - set(test))\n",
    "        # print type(test)\n",
    "        # print type(train)\n",
    "        random.shuffle(test)\n",
    "        random.shuffle(train)\n",
    "        return train, test\n",
    "\n",
    "\n",
    "    def main(filters = 128, filters_end=128,window=5,window_end=5, learning_rate = 0.01, dropout = 0.7, epochs = 10, batch_size = 100):\n",
    "        '''\n",
    "           Creates the NN model and train it with the batch data available in the input_training_folder file.\n",
    "        '''\n",
    "\n",
    "        # Global parameters\n",
    "        # Input folder that contains all the subfolders A,B,C...containing the batch files.\n",
    "        print(\"Parameters \" + str(filters) + \" \"+ str(filters_end) + \" \"+ str(window) + \" \"+ str(window_end) + \" \"+ str(learning_rate) + \" \"+ str(dropout) + str(epochs) + \" \" + str(batch_size))\n",
    "\n",
    "        input_folder = \"small_output/small_output\"  #In hops\n",
    "        \n",
    "        # Parameters: For the NN model, initially three parameters are going to be considered as a parameter.\n",
    "        # The number of filters, learning rate, dropout, epochs and batch size.\n",
    "          # batch_size should be a multiple of 10.\n",
    "        m = createModel(filters,filters_end,window,window_end, learning_rate, dropout)\n",
    "        metrics = m.metrics_names\n",
    "        print(str(metrics))\n",
    "\n",
    "        # Split training and testing files\n",
    "        all_files = []\n",
    "        for folder in os.listdir(input_folder):\n",
    "            print(str(folder))\n",
    "            for file in os.listdir(input_folder + \"/\" + folder):\n",
    "                if file.startswith(\"songs_\"):\n",
    "                    all_files.append(input_folder + \"/\" + folder + \"/\" + file)\n",
    "\n",
    "        training_files, test_files = train_test_split(all_files, test_size=0.3)\n",
    "        #     print(all_files)\n",
    "        #     print()\n",
    "        #     print(\"Training files: \" + str(training_files))\n",
    "        #     print()\n",
    "        #     print(\"Test files: \" + str(test_files))\n",
    "        #     print()\n",
    "        print(str(len(all_files)))\n",
    "        print(str(len(training_files)))\n",
    "        print(str(len(test_files)))\n",
    "\n",
    "        train_results_loss = []\n",
    "        train_results_acc = []\n",
    "        test_results_loss = []\n",
    "        test_results_acc = []\n",
    "\n",
    "        # We train the model. For each file in the training folder, we extract the batch, normalize it and then call the function\n",
    "        # train on batch from keras.\n",
    "        concatenations = int(batch_size / 10)\n",
    "        for epoch in range(epochs):\n",
    "            print(\"------- Executing epoch \" + str(epoch))\n",
    "            random.shuffle(training_files)\n",
    "            for i in range(0, len(training_files)):\n",
    "                # training_batch will store the final training batch of size batch_size. listY will do the same for the labels\n",
    "                listY = []\n",
    "                training_batch = np.empty(shape=(0, 132300))\n",
    "                count_labels = 0\n",
    "                for j in range(0, concatenations):\n",
    "                    # Get input and label data from the same batch\n",
    "                    if i<len(training_files) and \"songs_\" in training_files[i]:\n",
    "                        label = training_files[i].replace(\"songs\", \"labels\")\n",
    "                        print(\"training \" + training_files[i])\n",
    "                        data = np.load(str(training_files[i]))\n",
    "                        normalizedData = normalize(data, axis=0, order=2)\n",
    "                        print(str(normalizedData.shape))\n",
    "                        training_batch = np.concatenate((training_batch, normalizedData), axis=0)\n",
    "\n",
    "                        labels = np.load(str(label))\n",
    "\n",
    "                        # print(\"New Labels\")\n",
    "                        # print(labels)\n",
    "                        for d in labels:\n",
    "                            if(d == 10):\n",
    "                                listY.append([1])\n",
    "                                count_labels+=1\n",
    "                            else:\n",
    "                                listY.append([0])\n",
    "                    i += 1\n",
    "                print(\"training_batch shape\" + str(training_batch.shape))\n",
    "                print(\"training batch labels size \" + str(len(listY)))\n",
    "                print(str(count_labels))\n",
    "                #print(listY)\n",
    "\n",
    "                # As the first value of the training_batch is an initial array containing zeros, we start training from the\n",
    "                # first element of the array\n",
    "                training_batch = np.expand_dims(training_batch, axis=2)\n",
    "\n",
    "                x = m.train_on_batch(training_batch,listY)\n",
    "                print(\"New batch: Train Loss \" + str(x[0]) + \" Train accuracy \" + str(x[1]))\n",
    "                train_results_loss.append(x[0])\n",
    "                train_results_acc.append(x[1])\n",
    "\n",
    "            # We test the model. For each file in the test folder, we extract the batch, normalize it and then call the function\n",
    "            # test on batch from keras. Note that the test_on_batch function does not add to all batches, but it gives testing metrics\n",
    "            # per batch. Thus, it is necessary to add all the results.\n",
    "            # we only test every few batches (when running on hops we only need to test at the end)\n",
    "            # TODO: only test on last epoch on hops\n",
    "            if (epoch % 3 == 0 or epoch == epochs - 1):\n",
    "                total_loss = 0\n",
    "                total_accuracy = 0\n",
    "                total_test_data = 0\n",
    "\n",
    "                for file in test_files:\n",
    "                    if \"songs_\" in file:\n",
    "                        label = file.replace(\"songs\", \"labels\")\n",
    "                        print(\"test \" + file)\n",
    "                        x_test = np.load(file)\n",
    "                        x_test_normalized = normalize(x_test, axis=0, order=2)\n",
    "\n",
    "                        labels_test = np.load(label)\n",
    "                        y_test = []\n",
    "                        for d in labels_test:\n",
    "                            if(d == 10):\n",
    "                                y_test.append([1])\n",
    "                            else:\n",
    "                                y_test.append([0])\n",
    "\n",
    "                        x_test_normalized = np.expand_dims(x_test_normalized, axis=2)\n",
    "                        x = m.test_on_batch(x_test_normalized, y_test)\n",
    "\n",
    "                        print(\"New batch: Test Loss \" + str(x[0]) + \"Test accuracy \" + str(x[1]))\n",
    "                        total_test_data += 1\n",
    "                        total_loss += x[0]\n",
    "                        total_accuracy += x[1]\n",
    "\n",
    "                test_results_loss.append(total_loss / total_test_data)\n",
    "                test_results_acc.append(total_accuracy / total_test_data)\n",
    "                print(\"[Total test files] \" + str(total_test_data))\n",
    "                print(\"[total_loss] \" + str(total_loss) + \" [total_loss] \" + str(total_loss / total_test_data))\n",
    "                print(\"[total_accuracy] \" + str(total_accuracy) + \" [total_accuracy] \" + str(\n",
    "                    total_accuracy / total_test_data))\n",
    "\n",
    "        print(\"Final training loss: \")\n",
    "        print(str(train_results_loss))\n",
    "        print(\"Final training accuracy: \")\n",
    "        print(str(train_results_acc))\n",
    "        print(\"Final test loss: \")\n",
    "        print(str(test_results_loss))\n",
    "        print(\"Final test accuracy: \")\n",
    "        print(str(test_results_acc))\n",
    "\n",
    "        print(sep+str(total_accuracy/total_test_data)+sep)\n",
    "\n",
    "        #return total_accuracy/total_test_data\n",
    "\n",
    "\n",
    "    main(filters, filters_end,window,window_end, learning_rate, dropout, epochs, batch_size)\n",
    "    print(\"End time: \" + str(time.time()))\n",
    "    print(\"Elapsed time: \" + str(time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parallely executes and returns list of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(v):\n",
    "    if sep in v[\"_c0\"]:\n",
    "        i = v[\"_c0\"].find(sep)\n",
    "        substr = v[\"_c0\"][i+len(sep):]\n",
    "        i = substr.find(sep)\n",
    "        return [substr[:i]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_all_accuracies(tensorboard_hdfs_logdir, args_dict, number_params):\n",
    "    '''\n",
    "    Retrieves all accuracies from the parallel executions (each one is in a \n",
    "    different file, one per combination of wrapper function parameter)\n",
    "    '''\n",
    "    from hops import hdfs\n",
    "    print(tensorboard_hdfs_logdir)\n",
    "    hdfs.log(tensorboard_hdfs_logdir)\n",
    "    results=[]\n",
    "    \n",
    "    #Important, this must be ordered equally than _parse_to_dict function\n",
    "    population_dict = ['learning_rate', 'dropout',\n",
    "                          'num_steps','batch_size','filters','filters_end','kernel','kernel_end']\n",
    "    for i in range(number_params):\n",
    "        path_to_log=tensorboard_hdfs_logdir+\"/\"\n",
    "        for k in population_dict:\n",
    "            path_to_log+=k+\"=\"+str(args_dict[k][i])+\".\"\n",
    "        path_to_log+=\"log\"\n",
    "        print(\"Path to log: \")\n",
    "        hdfs.log(\"Path to log: \")\n",
    "        print(path_to_log)\n",
    "        hdfs.log(path_to_log)\n",
    "        raw = spark.read.csv(path_to_log, sep=\"\\n\")\n",
    "        \n",
    "        r = raw.rdd.flatMap(lambda v: get_accuracy(v)).collect()\n",
    "        results.extend(r)\n",
    "\n",
    "    #print(results)\n",
    "    return [float(res) for res in results]\n",
    "\n",
    "def execute_all(population_dict):\n",
    "    '''\n",
    "    Executes wrapper function with all values of population_dict parallely. \n",
    "    Returns a list of accuracies (or metric returned in the wrapper) in the \n",
    "    same order as in the population_dict.\n",
    "    '''\n",
    "    from hops import tflauncher\n",
    "    number_params=[len(v) for v in population_dict.values()][0]\n",
    "    tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, population_dict)\n",
    "    return get_all_accuracies(tensorboard_hdfs_logdir, population_dict,number_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolutionary algorithm for hyperparameter optimization\n",
    "To run code just adapt the last fuction (parse_to_dict) to include the items you wanna optimize.\n",
    "Also adapt the bounds and types in the main section to reflect the parameters you wanna optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Differential evolution algorithm extended to allow for categorical and integer values for optimization of hyperparameter\n",
    "space in Neural Networks, including an option for parallelization.\n",
    "\n",
    "This algorithm will create a full population to be evaluated, unlike typical differential evolution where each\n",
    "individual get compared and selected sequentially. This allows the user to send a whole population of parameters\n",
    "to a cluster and run computations in parallel, after which each individual gets evaluated with their respective\n",
    "target or trial vector.\n",
    "\n",
    "User will have to define:\n",
    "- Objective function to be optimized\n",
    "- Bounds of each parameter (all possible values)\n",
    "- The Types of each parameter, in order to be able to evaluate categorical, integer or floating values.\n",
    "- Direction of the optimization, i.e. maximization or minimization\n",
    "- Number of iterations, i.e. the amount of generations the algorithm will run\n",
    "- The population size, rule of thumb is to take between 5-10 time the amount of parameters to optimize\n",
    "- Mutation faction between [0, 2)\n",
    "- Crossover between [0, 1], the higher the value the more mutated values will crossover\n",
    "'''\n",
    "\n",
    "import random\n",
    "from hops import hdfs\n",
    "\n",
    "class DifferentialEvolution:\n",
    "    _types = ['float', 'int', 'cat']\n",
    "    _generation = 0\n",
    "    _scores = []\n",
    "\n",
    "    def __init__(self, objective_function, parbounds, types, direction = 'max', maxiter=10, popsize=10, mutationfactor=0.5, crossover=0.7):\n",
    "        self.objective_function = objective_function\n",
    "        self.parbounds = parbounds\n",
    "        self.direction = direction\n",
    "        self.types = types\n",
    "        self.maxiter = maxiter\n",
    "        self.n = popsize\n",
    "        self.F = mutationfactor\n",
    "        self.CR = crossover\n",
    "\n",
    "        #self.m = -1 if maximize else 1\n",
    "\n",
    "    # run differential evolution algorithms\n",
    "    def solve(self):\n",
    "        # initialise generation based on individual representation\n",
    "        population, bounds = self._population_initialisation()\n",
    "        hdfs.log(str(population))\n",
    "        print(str(population))\n",
    "        for _ in range(self.maxiter):\n",
    "            donor_population = self._mutation(population, bounds)\n",
    "            trial_population = self._recombination(population, donor_population)\n",
    "            population = self._selection(population, trial_population)\n",
    "\n",
    "            new_gen_avg = sum(self._scores)/self.n\n",
    "\n",
    "            if self.direction == 'max':\n",
    "                new_gen_best = max(self._scores)\n",
    "            else:\n",
    "                new_gen_best = min(self._scores)\n",
    "            new_gen_best_param = self._parse_back(population[self._scores.index(new_gen_best)])\n",
    "\n",
    "            hdfs.log(\"Generation: \" + str(self._generation) + \" || \" + \"Average score: \" + str(new_gen_avg)+\n",
    "                  \", best score: \" + str(new_gen_best) + \"best param: \" + str(new_gen_best_param))\n",
    "\n",
    "            print(\"Generation: \" + str(self._generation) + \" || \" + \"Average score: \" + str(new_gen_avg)+\n",
    "                  \", best score: \" + str(new_gen_best) + \"best param: \" + str(new_gen_best_param))\n",
    "\n",
    "        parsed_back_population = []\n",
    "        for indiv in population:\n",
    "            parsed_back_population.append(self._parse_back(indiv))\n",
    "\n",
    "        return parsed_back_population, self._scores\n",
    "\n",
    "    # define bounds of each individual depending on type\n",
    "    def _individual_representation(self):\n",
    "        bounds = []\n",
    "\n",
    "        for index, item in enumerate(self.types):\n",
    "            b =()\n",
    "            # if categorical then take bounds from 0 to number of items\n",
    "            if item == self._types[2]:\n",
    "                b = (0, int(len(self.parbounds[index]) - 1))\n",
    "            # if float/int then take given bounds\n",
    "            else:\n",
    "                b = self.parbounds[index]\n",
    "            bounds.append(b)\n",
    "        return bounds\n",
    "\n",
    "    # initialise population\n",
    "    def _population_initialisation(self):\n",
    "        population = []\n",
    "        num_parameters = len(self.parbounds)\n",
    "        for i in range(self.n):\n",
    "            indiv = []\n",
    "            bounds = self._individual_representation()\n",
    "\n",
    "            for i in range(num_parameters):\n",
    "                indiv.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "            indiv = self._ensure_bounds(indiv, bounds)\n",
    "            population.append(indiv)\n",
    "        return population, bounds\n",
    "\n",
    "    # ensure that any mutated individual is within bounds\n",
    "    def _ensure_bounds(self, indiv, bounds):\n",
    "        indiv_correct = []\n",
    "\n",
    "        for i in range(len(indiv)):\n",
    "            par = indiv[i]\n",
    "\n",
    "            # check if param is within bounds\n",
    "            lowerbound = bounds[i][0]\n",
    "            upperbound = bounds[i][1]\n",
    "            if par < lowerbound:\n",
    "                par = lowerbound\n",
    "            elif par > upperbound:\n",
    "                par = upperbound\n",
    "\n",
    "            # check if param needs rounding\n",
    "            if self.types[i] != 'float':\n",
    "                par = int(round(par))\n",
    "            indiv_correct.append(par)\n",
    "        return indiv_correct\n",
    "\n",
    "    # create donor population based on mutation of three vectors\n",
    "    def _mutation(self, population, bounds):\n",
    "        donor_population = []\n",
    "        for i in range(self.n):\n",
    "\n",
    "            indiv_indices = list(range(0, self.n))\n",
    "            indiv_indices.remove(i)\n",
    "\n",
    "            candidates = random.sample(indiv_indices, 3)\n",
    "            x_1 = population[candidates[0]]\n",
    "            x_2 = population[candidates[1]]\n",
    "            x_3 = population[candidates[2]]\n",
    "\n",
    "            # substracting the second from the third candidate\n",
    "            x_diff = [x_2_i - x_3_i for x_2_i, x_3_i in zip(x_2, x_3)]\n",
    "            donor_vec = [x_1_i + self.F*x_diff_i for x_1_i, x_diff_i in zip (x_1, x_diff)]\n",
    "            donor_vec = self._ensure_bounds(donor_vec, bounds)\n",
    "            donor_population.append(donor_vec)\n",
    "\n",
    "        return donor_population\n",
    "\n",
    "    # recombine donor vectors according to crossover probability\n",
    "    def _recombination(self, population, donor_population):\n",
    "        trial_population = []\n",
    "        for k in range(self.n):\n",
    "            target_vec = population[k]\n",
    "            donor_vec = donor_population[k]\n",
    "            trial_vec = []\n",
    "            for p in range(len(self.parbounds)):\n",
    "                crossover = random.random()\n",
    "\n",
    "                # if random number is below set crossover probability do recombination\n",
    "                if crossover <= self.CR:\n",
    "                    trial_vec.append(donor_vec[p])\n",
    "                else:\n",
    "                    trial_vec.append(target_vec[p])\n",
    "            trial_population.append(trial_vec)\n",
    "        return trial_population\n",
    "\n",
    "    # select the best individuals from each generation\n",
    "    def _selection(self, population, trial_population):\n",
    "        # Calculate trial vectors and target vectors and select next generation\n",
    "\n",
    "        if self._generation == 0:\n",
    "            parsed_population = []\n",
    "            for target_vec in population:\n",
    "                parsed_target_vec = self._parse_back(target_vec)\n",
    "                parsed_population.append(parsed_target_vec)\n",
    "\n",
    "            parsed_population = self._parse_to_dict(parsed_population)\n",
    "            self._scores = self.objective_function(parsed_population)\n",
    "\n",
    "        parsed_trial_population = []\n",
    "        for index, trial_vec in enumerate(trial_population):\n",
    "            parsed_trial_vec = self._parse_back(trial_vec)\n",
    "            parsed_trial_population.append(parsed_trial_vec)\n",
    "\n",
    "        parsed_trial_population =  self._parse_to_dict(parsed_trial_population)\n",
    "        trial_population_scores = self.objective_function(parsed_trial_population)\n",
    "\n",
    "        hdfs.log('Pop scores: ' + str(self._scores))\n",
    "        print('Pop scores: ' + str(self._scores))\n",
    "        hdfs.log('Trial scores: ' + str(trial_population_scores))\n",
    "        print('Trial scores: ' + str(trial_population_scores))\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            trial_vec_score_i = trial_population_scores[i]\n",
    "            target_vec_score_i = self._scores[i]\n",
    "            if self.direction == 'max':\n",
    "                if trial_vec_score_i > target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "            else:\n",
    "                if trial_vec_score_i < target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "\n",
    "        self._generation += 1\n",
    "\n",
    "        return population\n",
    "    # parse the converted values back to original\n",
    "    def _parse_back(self, individual):\n",
    "        original_representation = []\n",
    "        for index, parameter in enumerate(individual):\n",
    "            if self.types[index] == self._types[2]:\n",
    "                original_representation.append(self.parbounds[index][parameter])\n",
    "            else:\n",
    "\n",
    "                original_representation.append(parameter)\n",
    "\n",
    "        return original_representation\n",
    "\n",
    "    # for parallelization purposes one can parse the population from a list to a  dictionary format\n",
    "    # User only has to add the parameters he wants to optimize to population_dict\n",
    "    def _parse_to_dict(self, population):\n",
    "        population_dict = {'filters':[],'filters_end':[],'window':[],'window_end':[],'learning_rate': [], 'dropout': [],\n",
    "                          'epochs':[],'batch_size':[]}\n",
    "        for indiv in population:\n",
    "            population_dict['filters'].append(indiv[0])\n",
    "            population_dict['filters_end'].append(indiv[1])\n",
    "            population_dict['window'].append(indiv[2])\n",
    "            population_dict['window_end'].append(indiv[3])\n",
    "            population_dict['learning_rate'].append(indiv[4])\n",
    "            population_dict['dropout'].append(indiv[5])\n",
    "            population_dict['epochs'].append(indiv[6])\n",
    "            population_dict['batch_size'].append(indiv[7])\n",
    "\n",
    "        return population_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Observe that, for some combinations of parameters the network might exceed default RAM allocation on Hops\n",
    "\n",
    "#The parameters can be float, int or cat (categorical, tuple of values), this parameters must be specified in \n",
    "#function _parse_to_dict\n",
    "diff_evo = DifferentialEvolution(execute_all,\n",
    "                 [(50, 120),(30, 80),(5,20),(2,10),(0.001,0.02),(0.5,0.9),(3,20),(40,60,80,90,100)], \n",
    "                 ['int', 'int','int','int','float','float','int','cat'], \n",
    "                 direction='max', maxiter=10,popsize=8)\n",
    "\n",
    "results = diff_evo.solve()\n",
    "\n",
    "print(\"Population: \", results[0])\n",
    "print(\"Scores: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
