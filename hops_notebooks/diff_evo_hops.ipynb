{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Music NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4028</td><td>application_1513605045578_1395</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_1395/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop27:8042/node/containerlogs/container_e28_1513605045578_1395_01_000001/MarcMusicNN__diegorc0\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper_music(filters = 128, filters_end=128,window=5,window_end=5, learning_rate = 0.01, dropout = 0.7, epochs = 10, batch_size = 100):\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Input, GlobalMaxPooling1D, Dropout\n",
    "    from keras.utils import normalize\n",
    "    from keras import optimizers\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import keras\n",
    "    import random\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "\n",
    "    def createModel(filters,filters_end,window,window_end, learning_rate, dropout):\n",
    "        '''Creates the NN model. As an input, the NN expects a normalized batch of 10 132300-dimensional\n",
    "        For now, the following parameters are added to the model.\n",
    "        @:param filters: number of filters in all the conv1d layers {50, 300}\n",
    "        @:param learning_rate: learning rate of the model {0.0, 1.0}\n",
    "        @:param dropout: Dropout value {0.0, 1.0}\n",
    "            '''\n",
    "        model = Sequential()\n",
    "    #     model.add(Embedding(1,\n",
    "    #                         100,\n",
    "    #                         input_length=132300))\n",
    "    #     model.add(Dropout(dropout))\n",
    "        model.add(MaxPooling1D(5,input_shape = (132300,1)))\n",
    "        model.add(Conv1D(filters, window, activation='relu'))\n",
    "        model.add(MaxPooling1D(5))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Conv1D(filters, window, activation='relu'))\n",
    "        model.add(MaxPooling1D(5))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Conv1D(filters_end, window_end, activation='relu'))\n",
    "        model.add(MaxPooling1D(35))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        hdfs.log(str(model.summary()))\n",
    "\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train_test_split(all_files, test_size=0.3):\n",
    "        lenfiles = len(all_files)\n",
    "        lentest = int(lenfiles * test_size)\n",
    "        lentrain = lenfiles - lentest\n",
    "\n",
    "        test = random.sample(all_files, lentest)\n",
    "        train = list(set(all_files) - set(test))\n",
    "        # hdfs.log type(test)\n",
    "        # hdfs.log type(train)\n",
    "        random.shuffle(test)\n",
    "        random.shuffle(train)\n",
    "        return train, test\n",
    "\n",
    "\n",
    "    def main(filters = 128, filters_end=128,window=5,window_end=5, learning_rate = 0.01, dropout = 0.7, epochs = 10, batch_size = 100):\n",
    "        '''\n",
    "           Creates the NN model and train it with the batch data available in the input_training_folder file.\n",
    "        '''\n",
    "\n",
    "        # Global parameters\n",
    "        # Input folder that contains all the subfolders A,B,C...containing the batch files.\n",
    "\n",
    "        input_folder = \"raw_song_genre_dataset/output\"  #In hops\n",
    "        #input_folder = \"C:\\\\Users\\\\Diego\\\\Google Drive\\\\KTH\\\\Scalable Machine learning and deep learning\\\\project\\\\output\"\n",
    "        #input_folder=\"/home/mcr222/Documents/EIT/KTH/ScalableMachineLearning/MusicClassificationandGenerationusingDeepLearning/output_small\"\n",
    "\n",
    "        # Parameters: For the NN model, initially three parameters are going to be considered as a parameter.\n",
    "        # The number of filters, learning rate, dropout, epochs and batch size.\n",
    "          # batch_size should be a multiple of 10.\n",
    "        m = createModel(filters,filters_end,window,window_end, learning_rate, dropout)\n",
    "        metrics = m.metrics_names\n",
    "        hdfs.log(str(metrics))\n",
    "\n",
    "        # Split training and testing files\n",
    "        all_files = []\n",
    "        for folder in os.listdir(input_folder):\n",
    "            hdfs.log(str(folder))\n",
    "            for file in os.listdir(input_folder + \"/\" + folder):\n",
    "                if file.startswith(\"songs_\"):\n",
    "                    all_files.append(input_folder + \"/\" + folder + \"/\" + file)\n",
    "\n",
    "        training_files, test_files = train_test_split(all_files, test_size=0.3)\n",
    "        #     hdfs.log(all_files)\n",
    "        #     hdfs.log()\n",
    "        #     hdfs.log(\"Training files: \" + str(training_files))\n",
    "        #     hdfs.log()\n",
    "        #     hdfs.log(\"Test files: \" + str(test_files))\n",
    "        #     hdfs.log()\n",
    "        hdfs.log(str(len(all_files)))\n",
    "        hdfs.log(str(len(training_files)))\n",
    "        hdfs.log(str(len(test_files)))\n",
    "\n",
    "        train_results_loss = []\n",
    "        train_results_acc = []\n",
    "        test_results_loss = []\n",
    "        test_results_acc = []\n",
    "\n",
    "        # We train the model. For each file in the training folder, we extract the batch, normalize it and then call the function\n",
    "        # train on batch from keras.\n",
    "        concatenations = int(batch_size / 10)\n",
    "        for epoch in range(epochs):\n",
    "            hdfs.log(\"------- Executing epoch \" + str(epoch))\n",
    "            for i in range(0, len(training_files)):\n",
    "                # training_batch will store the final training batch of size batch_size. listY will do the same for the labels\n",
    "                listY = []\n",
    "                training_batch = np.empty(shape=(0, 132300))\n",
    "                count_labels = 0\n",
    "                for j in range(0, concatenations):\n",
    "                    # Get input and label data from the same batch\n",
    "                    if i<len(training_files) and \"songs_\" in training_files[i]:\n",
    "                        label = training_files[i].replace(\"songs\", \"labels\")\n",
    "                        hdfs.log(\"training \" + training_files[i])\n",
    "                        data = np.load(str(training_files[i]))\n",
    "                        normalizedData = normalize(data, axis=0, order=2)\n",
    "                        hdfs.log(str(normalizedData.shape))\n",
    "                        training_batch = np.concatenate((training_batch, normalizedData), axis=0)\n",
    "\n",
    "                        labels = np.load(str(label))\n",
    "\n",
    "                        # hdfs.log(\"New Labels\")\n",
    "                        # hdfs.log(labels)\n",
    "                        #TODO: change for np.reshape(a,(3,1))\n",
    "                        for d in labels:\n",
    "                            if(d == 10):\n",
    "                                listY.append([1])\n",
    "                                count_labels+=1\n",
    "                            else:\n",
    "                                listY.append([0])\n",
    "                    i += 1\n",
    "                hdfs.log(\"training_batch shape\" + str(training_batch.shape))\n",
    "                hdfs.log(\"training batch labels size \" + str(len(listY)))\n",
    "                hdfs.log(str(count_labels))\n",
    "                #hdfs.log(listY)\n",
    "\n",
    "                # As the first value of the training_batch is an initial array containing zeros, we start training from the\n",
    "                # first element of the array\n",
    "                training_batch = np.expand_dims(training_batch, axis=2)\n",
    "\n",
    "                x = m.train_on_batch(training_batch,listY)\n",
    "                hdfs.log(\"New batch: Train Loss \" + str(x[0]) + \" Train accuracy \" + str(x[1]))\n",
    "                train_results_loss.append(x[0])\n",
    "                train_results_acc.append(x[1])\n",
    "\n",
    "            # We test the model. For each file in the test folder, we extract the batch, normalize it and then call the function\n",
    "            # test on batch from keras. Note that the test_on_batch function does not add to all batches, but it gives testing metrics\n",
    "            # per batch. Thus, it is necessary to add all the results.\n",
    "            # we only test every few batches (when running on hops we only need to test at the end)\n",
    "            # TODO: only test on last epoch on hops\n",
    "            if (epoch % 3 == 0 or epoch == epochs - 1):\n",
    "                total_loss = 0\n",
    "                total_accuracy = 0\n",
    "                total_test_data = 0\n",
    "\n",
    "                for file in test_files:\n",
    "                    if \"songs_\" in file:\n",
    "                        label = file.replace(\"songs\", \"labels\")\n",
    "                        hdfs.log(\"test \" + file)\n",
    "                        x_test = np.load(file)\n",
    "                        x_test_normalized = normalize(x_test, axis=0, order=2)\n",
    "\n",
    "                        labels_test = np.load(label)\n",
    "                        y_test = []\n",
    "                        for d in labels_test:\n",
    "                            if(d == 10):\n",
    "                                y_test.append([1])\n",
    "                            else:\n",
    "                                y_test.append([0])\n",
    "\n",
    "                        x_test_normalized = np.expand_dims(x_test_normalized, axis=2)\n",
    "                        x = m.test_on_batch(x_test_normalized, y_test)\n",
    "\n",
    "                        hdfs.log(\"New batch: Test Loss \" + str(x[0]) + \"Test accuracy \" + str(x[1]))\n",
    "                        total_test_data += 1\n",
    "                        total_loss += x[0]\n",
    "                        total_accuracy += x[1]\n",
    "\n",
    "                test_results_loss.append(total_loss / total_test_data)\n",
    "                test_results_acc.append(total_accuracy / total_test_data)\n",
    "                hdfs.log(\"[Total test files] \" + str(total_test_data))\n",
    "                hdfs.log(\"[total_loss] \" + str(total_loss) + \" [total_loss] \" + str(total_loss / total_test_data))\n",
    "                hdfs.log(\"[total_accuracy] \" + str(total_accuracy) + \" [total_accuracy] \" + str(\n",
    "                    total_accuracy / total_test_data))\n",
    "\n",
    "        hdfs.log(\"Final training loss: \")\n",
    "        hdfs.log(str(train_results_loss))\n",
    "        hdfs.log(\"Final training accuracy: \")\n",
    "        hdfs.log(str(train_results_acc))\n",
    "        hdfs.log(\"Final test loss: \")\n",
    "        hdfs.log(str(test_results_loss))\n",
    "        hdfs.log(\"Final test accuracy: \")\n",
    "        hdfs.log(str(test_results_acc))\n",
    "\n",
    "        hdfs.log(sep+str(total_accuracy/total_test_data)+sep)\n",
    "\n",
    "        #return total_accuracy/total_test_data\n",
    "\n",
    "\n",
    "    main(filters, filters_end,window,window_end, learning_rate, dropout, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fashion MNIST\n",
    "Requires to have the tfrecords (on github, hops_notebooks folder) in a folder in datasets called mnist (to be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper_mnist(learning_rate, dropout):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "\n",
    "    # Training Parameters\n",
    "    num_steps = 20\n",
    "    batch_size = 128\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    train_filenames = [hdfs.project_path() + \"mnist/train.tfrecords\"]\n",
    "    validation_filenames = [hdfs.project_path() + \"mnist/validation.tfrecords\"]\n",
    "\n",
    "    # Create the neural network\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "\n",
    "        # Define a scope for reusing the variables\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\n",
    "            # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "            # Reshape to match picture format [Height x Width x Channel]\n",
    "            # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "            x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "            # Flatten the data to a 1-D vector for the fully connected layer\n",
    "            fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            # Fully connected layer (in tf contrib folder for now)\n",
    "            fc1 = tf.layers.dense(fc1, 1024)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "            # Output layer, class prediction\n",
    "            out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode, params):\n",
    "\n",
    "        # Build the neural network\n",
    "        # Because Dropout have different behavior at training and prediction time, we\n",
    "        # need to create 2 distinct computation graphs that still share the same weights.\n",
    "        logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "        logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits_test, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, \n",
    "                                                                                labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "        image = tf.reshape(features[:10], [-1, 28, 28, 1])\n",
    "        tf.summary.image(\"image\", image)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "        return estim_specs\n",
    "\n",
    "\n",
    "    def data_input_fn(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "        def parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "            image.set_shape([28 * 28])\n",
    "\n",
    "            # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "            image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "            return image, label\n",
    "\n",
    "        def _input_fn():\n",
    "            # Import MNIST data\n",
    "            dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Map the parser over dataset, and batch results by up to batch_size\n",
    "            dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(buffer_size=128)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.repeat(repeat)\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "            features, labels = iterator.get_next()\n",
    "\n",
    "            return features, labels\n",
    "\n",
    "        return _input_fn\n",
    "\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=tensorboard.logdir(),\n",
    "        save_checkpoints_steps=10,\n",
    "        save_summary_steps=5,\n",
    "        log_step_count_steps=10)\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "          save_steps = run_config.save_summary_steps,\n",
    "          scaffold= tf.train.Scaffold(),\n",
    "          summary_op=tf.summary.merge_all())\n",
    "\n",
    "    mnist_estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "\n",
    "\n",
    "    train_input_fn = data_input_fn(train_filenames[0], batch_size=batch_size)\n",
    "    eval_input_fn = data_input_fn(validation_filenames[0], batch_size=batch_size)\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        mnist_estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=num_steps,\n",
    "        min_eval_frequency=5,\n",
    "        eval_hooks=[summary_hook]\n",
    "    )\n",
    "    \n",
    "    hdfs.log(\"Execution train and evaluate\")\n",
    "    experiment.train_and_evaluate()\n",
    "    hdfs.log(\"Finished execution train and evaluate\")\n",
    "    #accuracy_score = mnist_estimator.evaluate(input_fn=eval_input_fn)[\"accuracy\"]\n",
    "\n",
    "    #accuracy_score = experiment.evaluate()\n",
    "    #hdfs.log(\"Variable names: \")\n",
    "    #var = mnist_estimator.get_variable_names()\n",
    "    #for aux in var:\n",
    "        #hdfs.log(aux)\n",
    "    hdfs.log(\"Trying estimator evaluate: \")\n",
    "    accuracy_score = mnist_estimator.evaluate(input_fn=eval_input_fn, steps=num_steps)[\"accuracy\"]\n",
    "    hdfs.log(\"Done estimator evaluate: \")\n",
    "    hdfs.log(sep+str(accuracy_score)+sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from hops import tflauncher\n",
    "#import timeit\n",
    "#args_dict = {'learning_rate': [0.0005,0.02], 'dropout': [0.7,0.03]} \n",
    "#start_time = timeit.default_timer()\n",
    "#tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, args_dict)\n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#print \"Elapsed time: \" + str(elapsed)\n",
    "\n",
    "#start_time = timeit.default_timer()\n",
    "#tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, args_dict)\n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#print \"Elapsed time: \" + str(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Music ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper(learning_rate, dropout):\n",
    "    from hops import hdfs\n",
    "    #TODO: add network here with all the import within necessary for the code within the function\n",
    "    \n",
    "    acc=0.87\n",
    "    hdfs.log(sep+str(acc)+sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parallely executes and returns list of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(v):\n",
    "    if sep in v[\"_c0\"]:\n",
    "        i = v[\"_c0\"].find(sep)\n",
    "        substr = v[\"_c0\"][i+len(sep):]\n",
    "        i = substr.find(sep)\n",
    "        return [substr[:i]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_all_accuracies(tensorboard_hdfs_logdir, args_dict, number_params):\n",
    "    from hops import hdfs\n",
    "    print(tensorboard_hdfs_logdir)\n",
    "    hdfs.log(tensorboard_hdfs_logdir)\n",
    "    results=[]\n",
    "    for i in range(number_params):\n",
    "        path_to_log=tensorboard_hdfs_logdir+\"/\"\n",
    "        for k in args_dict.keys():\n",
    "            path_to_log+=k+\"=\"+str(args_dict[k][i])+\".\"\n",
    "        path_to_log+=\"log\"\n",
    "        print(\"Path to log: \")\n",
    "        hdfs.log(\"Path to log: \")\n",
    "        print(path_to_log)\n",
    "        hdfs.log(path_to_log)\n",
    "        raw = spark.read.csv(path_to_log, sep=\"\\n\")\n",
    "        #raw.show(10)\n",
    "        #raw.count()\n",
    "        r = raw.rdd.flatMap(lambda v: get_accuracy(v)).collect()\n",
    "        results.extend(r)\n",
    "\n",
    "    #print(results)\n",
    "    return [float(res) for res in results]\n",
    "\n",
    "def execute_all(population_dict):\n",
    "    from hops import tflauncher\n",
    "    number_params=[len(v) for v in population_dict.values()][0]\n",
    "    tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_music, population_dict)\n",
    "    return get_all_accuracies(tensorboard_hdfs_logdir, population_dict,number_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolutionary algorithm for hyperparameter optimization\n",
    "To run code just adapt the last fuction (parse_to_dict) to include the items you wanna optimize\n",
    "Also adapt the bounds and types in the main section to reflect the parameters you wanna optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Differential evolution algorithm extended to allow for categorical and integer values for optimization of hyperparameter\n",
    "space in Neural Networks, including an option for parallelization.\n",
    "\n",
    "This algorithm will create a full population to be evaluated, unlike typical differential evolution where each\n",
    "individual get compared and selected sequentially. This allows the user to send a whole population of parameters\n",
    "to a cluster and run computations in parallel, after which each individual gets evaluated with their respective\n",
    "target or trial vector.\n",
    "\n",
    "User will have to define:\n",
    "- Objective function to be optimized\n",
    "- Bounds of each parameter (all possible values)\n",
    "- The Types of each parameter, in order to be able to evaluate categorical, integer or floating values.\n",
    "- Direction of the optimization, i.e. maximization or minimization\n",
    "- Number of iterations, i.e. the amount of generations the algorithm will run\n",
    "- The population size, rule of thumb is to take between 5-10 time the amount of parameters to optimize\n",
    "- Mutation faction between [0, 2)\n",
    "- Crossover between [0, 1], the higher the value the more mutated values will crossover\n",
    "'''\n",
    "\n",
    "import random\n",
    "from hops import hdfs\n",
    "\n",
    "class DifferentialEvolution:\n",
    "    _types = ['float', 'int', 'cat']\n",
    "    _generation = 0\n",
    "    _scores = []\n",
    "\n",
    "    def __init__(self, objective_function, parbounds, types, direction = 'max', maxiter=10, popsize=10, mutationfactor=0.5, crossover=0.7):\n",
    "        self.objective_function = objective_function\n",
    "        self.parbounds = parbounds\n",
    "        self.direction = direction\n",
    "        self.types = types\n",
    "        self.maxiter = maxiter\n",
    "        self.n = popsize\n",
    "        self.F = mutationfactor\n",
    "        self.CR = crossover\n",
    "\n",
    "        #self.m = -1 if maximize else 1\n",
    "\n",
    "    # run differential evolution algorithms\n",
    "    def solve(self):\n",
    "        # initialise generation based on individual representation\n",
    "        population, bounds = self._population_initialisation()\n",
    "        hdfs.log(population)\n",
    "        for _ in range(self.maxiter):\n",
    "            donor_population = self._mutation(population, bounds)\n",
    "            trial_population = self._recombination(population, donor_population)\n",
    "            population = self._selection(population, trial_population)\n",
    "\n",
    "            new_gen_avg = sum(self._scores)/self.n\n",
    "\n",
    "            if self.direction == 'max':\n",
    "                new_gen_best = max(self._scores)\n",
    "            else:\n",
    "                new_gen_best = min(self._scores)\n",
    "            new_gen_best_param = self._parse_back(population[self._scores.index(new_gen_best)])\n",
    "\n",
    "            hdfs.log(\"Generation: \", self._generation, \" || \", \"Average score: \", new_gen_avg,\n",
    "                  \", best score: \", new_gen_best, \"best param: \", new_gen_best_param)\n",
    "\n",
    "        parsed_back_population = []\n",
    "        for indiv in population:\n",
    "            parsed_back_population.append(self._parse_back(indiv))\n",
    "\n",
    "        return parsed_back_population, self._scores\n",
    "\n",
    "    # define bounds of each individual depending on type\n",
    "    def _individual_representation(self):\n",
    "        bounds = []\n",
    "\n",
    "        for index, item in enumerate(self.types):\n",
    "            b =()\n",
    "            # if categorical then take bounds from 0 to number of items\n",
    "            if item == self._types[2]:\n",
    "                b = (0, int(len(self.parbounds[index]) - 1))\n",
    "            # if float/int then take given bounds\n",
    "            else:\n",
    "                b = self.parbounds[index]\n",
    "            bounds.append(b)\n",
    "        return bounds\n",
    "\n",
    "    # initialise population\n",
    "    def _population_initialisation(self):\n",
    "        population = []\n",
    "        num_parameters = len(self.parbounds)\n",
    "        for i in range(self.n):\n",
    "            indiv = []\n",
    "            bounds = self._individual_representation()\n",
    "\n",
    "            for i in range(num_parameters):\n",
    "                indiv.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "            indiv = self._ensure_bounds(indiv, bounds)\n",
    "            population.append(indiv)\n",
    "        return population, bounds\n",
    "\n",
    "    # ensure that any mutated individual is within bounds\n",
    "    def _ensure_bounds(self, indiv, bounds):\n",
    "        indiv_correct = []\n",
    "\n",
    "        for i in range(len(indiv)):\n",
    "            par = indiv[i]\n",
    "\n",
    "            # check if param is within bounds\n",
    "            lowerbound = bounds[i][0]\n",
    "            upperbound = bounds[i][1]\n",
    "            if par < lowerbound:\n",
    "                par = lowerbound\n",
    "            elif par > upperbound:\n",
    "                par = upperbound\n",
    "\n",
    "            # check if param needs rounding\n",
    "            if self.types[i] != 'float':\n",
    "                par = int(round(par))\n",
    "            indiv_correct.append(par)\n",
    "        return indiv_correct\n",
    "\n",
    "    # create donor population based on mutation of three vectors\n",
    "    def _mutation(self, population, bounds):\n",
    "        donor_population = []\n",
    "        for i in range(self.n):\n",
    "\n",
    "            indiv_indices = list(range(0, self.n))\n",
    "            indiv_indices.remove(i)\n",
    "\n",
    "            candidates = random.sample(indiv_indices, 3)\n",
    "            x_1 = population[candidates[0]]\n",
    "            x_2 = population[candidates[1]]\n",
    "            x_3 = population[candidates[2]]\n",
    "\n",
    "            # substracting the second from the third candidate\n",
    "            x_diff = [x_2_i - x_3_i for x_2_i, x_3_i in zip(x_2, x_3)]\n",
    "            donor_vec = [x_1_i + self.F*x_diff_i for x_1_i, x_diff_i in zip (x_1, x_diff)]\n",
    "            donor_vec = self._ensure_bounds(donor_vec, bounds)\n",
    "            donor_population.append(donor_vec)\n",
    "\n",
    "        return donor_population\n",
    "\n",
    "    # recombine donor vectors according to crossover probability\n",
    "    def _recombination(self, population, donor_population):\n",
    "        trial_population = []\n",
    "        for k in range(self.n):\n",
    "            target_vec = population[k]\n",
    "            donor_vec = donor_population[k]\n",
    "            trial_vec = []\n",
    "            for p in range(len(self.parbounds)):\n",
    "                crossover = random.random()\n",
    "\n",
    "                # if random number is below set crossover probability do recombination\n",
    "                if crossover <= self.CR:\n",
    "                    trial_vec.append(donor_vec[p])\n",
    "                else:\n",
    "                    trial_vec.append(target_vec[p])\n",
    "            trial_population.append(trial_vec)\n",
    "        return trial_population\n",
    "\n",
    "    # select the best individuals from each generation\n",
    "    def _selection(self, population, trial_population):\n",
    "        # Calculate trial vectors and target vectors and select next generation\n",
    "\n",
    "        if self._generation == 0:\n",
    "            parsed_population = []\n",
    "            for target_vec in population:\n",
    "                parsed_target_vec = self._parse_back(target_vec)\n",
    "                parsed_population.append(parsed_target_vec)\n",
    "\n",
    "            parsed_population = self._parse_to_dict(parsed_population)\n",
    "            self._scores = self.objective_function(parsed_population)\n",
    "\n",
    "        parsed_trial_population = []\n",
    "        for index, trial_vec in enumerate(trial_population):\n",
    "            parsed_trial_vec = self._parse_back(trial_vec)\n",
    "            parsed_trial_population.append(parsed_trial_vec)\n",
    "\n",
    "        parsed_trial_population =  self._parse_to_dict(parsed_trial_population)\n",
    "        trial_population_scores = self.objective_function(parsed_trial_population)\n",
    "\n",
    "        hdfs.log('Pop scores: ', self._scores)\n",
    "        hdfs.log('Trial scores: ', trial_population_scores)\n",
    "        for i in range(self.n):\n",
    "            trial_vec_score_i = trial_population_scores[i]\n",
    "            target_vec_score_i = self._scores[i]\n",
    "            if self.direction == 'max':\n",
    "                if trial_vec_score_i > target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "            else:\n",
    "                if trial_vec_score_i < target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "\n",
    "        self._generation += 1\n",
    "\n",
    "        return population\n",
    "    # parse the converted values back to original\n",
    "    def _parse_back(self, individual):\n",
    "        original_representation = []\n",
    "        for index, parameter in enumerate(individual):\n",
    "            if self.types[index] == self._types[2]:\n",
    "                original_representation.append(self.parbounds[index][parameter])\n",
    "            else:\n",
    "\n",
    "                original_representation.append(parameter)\n",
    "\n",
    "        return original_representation\n",
    "\n",
    "    # for parallelization purposes one can parse the population from a list to a  dictionary format\n",
    "    # User only has to add the parameters he wants to optimize to population_dict\n",
    "    def _parse_to_dict(self, population):\n",
    "        population_dict = {'filters':[],'filters_end':[],'window':[],'window_end':[],'learning_rate': [], 'dropout': [],\n",
    "                          'epochs':[],'batch_size':[]}\n",
    "        for indiv in population:\n",
    "            population_dict['filters'].append(indiv[0])\n",
    "            population_dict['filters_end'].append(indiv[1])\n",
    "            population_dict['window'].append(indiv[2])\n",
    "            population_dict['window_end'].append(indiv[3])\n",
    "            population_dict['learning_rate'].append(indiv[4])\n",
    "            population_dict['dropout'].append(indiv[5])\n",
    "            population_dict['epochs'].append(indiv[6])\n",
    "            population_dict['batch_size'].append(indiv[7])\n",
    "\n",
    "        return population_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diff_evo = DifferentialEvolution(execute_all,\n",
    "                 [(50, 120),(30, 80),(5,20),(2,10),(0.001,0.02),(0.5,0.9),(3,20),(40,60,80,90,100)], \n",
    "                 ['int', 'int','int','int','float','float','int','cat'], \n",
    "                 direction='max', maxiter=2,popsize=4)\n",
    "\n",
    "results = diff_evo.solve()\n",
    "\n",
    "print(\"Population: \", results[0])\n",
    "print(\"Scores: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
