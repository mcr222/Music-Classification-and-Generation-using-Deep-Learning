{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fashion MNIST\n",
    "Requires to have the tfrecords (on github, hops_notebooks folder) in a folder in datasets called mnist (to be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3318</td><td>application_1513605045578_0591</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_0591/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop26:8042/node/containerlogs/container_e28_1513605045578_0591_01_000001/MarcMusicClass__marccr00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper_mnist(learning_rate, dropout):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "\n",
    "    # Training Parameters\n",
    "    num_steps = 20\n",
    "    batch_size = 128\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    train_filenames = [hdfs.project_path() + \"mnist/train.tfrecords\"]\n",
    "    validation_filenames = [hdfs.project_path() + \"mnist/validation.tfrecords\"]\n",
    "\n",
    "    # Create the neural network\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "\n",
    "        # Define a scope for reusing the variables\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\n",
    "            # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "            # Reshape to match picture format [Height x Width x Channel]\n",
    "            # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "            x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "            # Flatten the data to a 1-D vector for the fully connected layer\n",
    "            fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            # Fully connected layer (in tf contrib folder for now)\n",
    "            fc1 = tf.layers.dense(fc1, 1024)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "            # Output layer, class prediction\n",
    "            out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode, params):\n",
    "\n",
    "        # Build the neural network\n",
    "        # Because Dropout have different behavior at training and prediction time, we\n",
    "        # need to create 2 distinct computation graphs that still share the same weights.\n",
    "        logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "        logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits_test, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, \n",
    "                                                                                labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "        image = tf.reshape(features[:10], [-1, 28, 28, 1])\n",
    "        tf.summary.image(\"image\", image)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "        return estim_specs\n",
    "\n",
    "\n",
    "    def data_input_fn(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "        def parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "            image.set_shape([28 * 28])\n",
    "\n",
    "            # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "            image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "            return image, label\n",
    "\n",
    "        def _input_fn():\n",
    "            # Import MNIST data\n",
    "            dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Map the parser over dataset, and batch results by up to batch_size\n",
    "            dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(buffer_size=128)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.repeat(repeat)\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "            features, labels = iterator.get_next()\n",
    "\n",
    "            return features, labels\n",
    "\n",
    "        return _input_fn\n",
    "\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=tensorboard.logdir(),\n",
    "        save_checkpoints_steps=10,\n",
    "        save_summary_steps=5,\n",
    "        log_step_count_steps=10)\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "          save_steps = run_config.save_summary_steps,\n",
    "          scaffold= tf.train.Scaffold(),\n",
    "          summary_op=tf.summary.merge_all())\n",
    "\n",
    "    mnist_estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "\n",
    "\n",
    "    train_input_fn = data_input_fn(train_filenames[0], batch_size=batch_size)\n",
    "    eval_input_fn = data_input_fn(validation_filenames[0], batch_size=batch_size)\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        mnist_estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=num_steps,\n",
    "        min_eval_frequency=5,\n",
    "        eval_hooks=[summary_hook]\n",
    "    )\n",
    "    \n",
    "    hdfs.log(\"Execution train and evaluate\")\n",
    "    experiment.train_and_evaluate()\n",
    "    hdfs.log(\"Finished execution train and evaluate\")\n",
    "    #accuracy_score = mnist_estimator.evaluate(input_fn=eval_input_fn)[\"accuracy\"]\n",
    "\n",
    "    #accuracy_score = experiment.evaluate()\n",
    "    #hdfs.log(\"Variable names: \")\n",
    "    #var = mnist_estimator.get_variable_names()\n",
    "    #for aux in var:\n",
    "        #hdfs.log(aux)\n",
    "    hdfs.log(\"Trying estimator evaluate: \")\n",
    "    accuracy_score = mnist_estimator.evaluate(input_fn=eval_input_fn, steps=num_steps)[\"accuracy\"]\n",
    "    hdfs.log(\"Done estimator evaluate: \")\n",
    "    hdfs.log(sep+str(accuracy_score)+sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from hops import tflauncher\n",
    "#import timeit\n",
    "#args_dict = {'learning_rate': [0.0005,0.02], 'dropout': [0.7,0.03]} \n",
    "#start_time = timeit.default_timer()\n",
    "#tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, args_dict)\n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#print \"Elapsed time: \" + str(elapsed)\n",
    "\n",
    "#start_time = timeit.default_timer()\n",
    "#tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, args_dict)\n",
    "#elapsed = timeit.default_timer() - start_time\n",
    "#print \"Elapsed time: \" + str(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Music ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper(learning_rate, dropout):\n",
    "    from hops import hdfs\n",
    "    #TODO: add network here with all the import within necessary for the code within the function\n",
    "    \n",
    "    acc=0.87\n",
    "    hdfs.log(sep+str(acc)+sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parallely executes and returns list of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(v):\n",
    "    if sep in v[\"_c0\"]:\n",
    "        i = v[\"_c0\"].find(sep)\n",
    "        substr = v[\"_c0\"][i+len(sep):]\n",
    "        i = substr.find(sep)\n",
    "        return [substr[:i]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_all_accuracies(tensorboard_hdfs_logdir, args_dict, number_params):\n",
    "    from hops import hdfs\n",
    "    print(tensorboard_hdfs_logdir)\n",
    "    hdfs.log(tensorboard_hdfs_logdir)\n",
    "    results=[]\n",
    "    for i in range(number_params):\n",
    "        path_to_log=tensorboard_hdfs_logdir+\"/\"\n",
    "        for k in args_dict.keys():\n",
    "            path_to_log+=k+\"=\"+str(args_dict[k][i])+\".\"\n",
    "        path_to_log+=\"log\"\n",
    "        print(\"Path to log: \")\n",
    "        hdfs.log(\"Path to log: \")\n",
    "        print(path_to_log)\n",
    "        hdfs.log(path_to_log)\n",
    "        raw = spark.read.csv(path_to_log, sep=\"\\n\")\n",
    "        #raw.show(10)\n",
    "        #raw.count()\n",
    "        r = raw.rdd.flatMap(lambda v: get_accuracy(v)).collect()\n",
    "        results.extend(r)\n",
    "\n",
    "    #print(results)\n",
    "    return [float(res) for res in results]\n",
    "\n",
    "def execute_all(population_dict):\n",
    "    from hops import tflauncher\n",
    "    number_params=[len(v) for v in population_dict.values()][0]\n",
    "    tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, population_dict)\n",
    "    return get_all_accuracies(tensorboard_hdfs_logdir, population_dict,number_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolutionary algorithm for hyperparameter optimization\n",
    "To run code just adapt the last fuction (parse_to_dict) to include the items you wanna optimize\n",
    "Also adapt the bounds and types in the main section to reflect the parameters you wanna optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Differential evolution algorithm extended to allow for categorical and integer values for optimization of hyperparameter\n",
    "space in Neural Networks, including an option for parallelization.\n",
    "\n",
    "This algorithm will create a full population to be evaluated, unlike typical differential evolution where each\n",
    "individual get compared and selected sequentially. This allows the user to send a whole population of parameters\n",
    "to a cluster and run computations in parallel, after which each individual gets evaluated with their respective\n",
    "target or trial vector.\n",
    "\n",
    "User will have to define:\n",
    "- Objective function to be optimized\n",
    "- Bounds of each parameter (all possible values)\n",
    "- The Types of each parameter, in order to be able to evaluate categorical, integer or floating values.\n",
    "- Direction of the optimization, i.e. maximization or minimization\n",
    "- Number of iterations, i.e. the amount of generations the algorithm will run\n",
    "- The population size, rule of thumb is to take between 5-10 time the amount of parameters to optimize\n",
    "- Mutation faction between [0, 2)\n",
    "- Crossover between [0, 1], the higher the value the more mutated values will crossover\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "class DifferentialEvolution:\n",
    "    _types = ['float', 'int', 'cat']\n",
    "    _generation = 0\n",
    "    _scores = []\n",
    "\n",
    "    def __init__(self, objective_function, parbounds, types, direction = 'max', maxiter=10, popsize=10, mutationfactor=0.5, crossover=0.7):\n",
    "        self.objective_function = objective_function\n",
    "        self.parbounds = parbounds\n",
    "        self.direction = direction\n",
    "        self.types = types\n",
    "        self.maxiter = maxiter\n",
    "        self.n = popsize\n",
    "        self.F = mutationfactor\n",
    "        self.CR = crossover\n",
    "\n",
    "        #self.m = -1 if maximize else 1\n",
    "\n",
    "    # run differential evolution algorithms\n",
    "    def solve(self):\n",
    "        # initialise generation based on individual representation\n",
    "        population, bounds = self._population_initialisation()\n",
    "        print(population)\n",
    "        for _ in range(self.maxiter):\n",
    "            donor_population = self._mutation(population, bounds)\n",
    "            trial_population = self._recombination(population, donor_population)\n",
    "            population = self._selection(population, trial_population)\n",
    "\n",
    "            new_gen_avg = sum(self._scores)/self.n\n",
    "\n",
    "            if self.direction == 'max':\n",
    "                new_gen_best = max(self._scores)\n",
    "            else:\n",
    "                new_gen_best = min(self._scores)\n",
    "            new_gen_best_param = self._parse_back(population[self._scores.index(new_gen_best)])\n",
    "\n",
    "            print(\"Generation: \", self._generation, \" || \", \"Average score: \", new_gen_avg,\n",
    "                  \", best score: \", new_gen_best, \"best param: \", new_gen_best_param)\n",
    "\n",
    "        parsed_back_population = []\n",
    "        for indiv in population:\n",
    "            parsed_back_population.append(self._parse_back(indiv))\n",
    "\n",
    "        return parsed_back_population, self._scores\n",
    "\n",
    "    # define bounds of each individual depending on type\n",
    "    def _individual_representation(self):\n",
    "        bounds = []\n",
    "\n",
    "        for index, item in enumerate(self.types):\n",
    "            b =()\n",
    "            # if categorical then take bounds from 0 to number of items\n",
    "            if item == self._types[2]:\n",
    "                b = (0, int(len(self.parbounds[index]) - 1))\n",
    "            # if float/int then take given bounds\n",
    "            else:\n",
    "                b = self.parbounds[index]\n",
    "            bounds.append(b)\n",
    "        return bounds\n",
    "\n",
    "    # initialise population\n",
    "    def _population_initialisation(self):\n",
    "        population = []\n",
    "        num_parameters = len(self.parbounds)\n",
    "        for i in range(self.n):\n",
    "            indiv = []\n",
    "            bounds = self._individual_representation()\n",
    "\n",
    "            for i in range(num_parameters):\n",
    "                indiv.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "            indiv = self._ensure_bounds(indiv, bounds)\n",
    "            population.append(indiv)\n",
    "        return population, bounds\n",
    "\n",
    "    # ensure that any mutated individual is within bounds\n",
    "    def _ensure_bounds(self, indiv, bounds):\n",
    "        indiv_correct = []\n",
    "\n",
    "        for i in range(len(indiv)):\n",
    "            par = indiv[i]\n",
    "\n",
    "            # check if param is within bounds\n",
    "            lowerbound = bounds[i][0]\n",
    "            upperbound = bounds[i][1]\n",
    "            if par < lowerbound:\n",
    "                par = lowerbound\n",
    "            elif par > upperbound:\n",
    "                par = upperbound\n",
    "\n",
    "            # check if param needs rounding\n",
    "            if self.types[i] != 'float':\n",
    "                par = int(round(par))\n",
    "            indiv_correct.append(par)\n",
    "        return indiv_correct\n",
    "\n",
    "    # create donor population based on mutation of three vectors\n",
    "    def _mutation(self, population, bounds):\n",
    "        donor_population = []\n",
    "        for i in range(self.n):\n",
    "\n",
    "            indiv_indices = list(range(0, self.n))\n",
    "            indiv_indices.remove(i)\n",
    "\n",
    "            candidates = random.sample(indiv_indices, 3)\n",
    "            x_1 = population[candidates[0]]\n",
    "            x_2 = population[candidates[1]]\n",
    "            x_3 = population[candidates[2]]\n",
    "\n",
    "            # substracting the second from the third candidate\n",
    "            x_diff = [x_2_i - x_3_i for x_2_i, x_3_i in zip(x_2, x_3)]\n",
    "            donor_vec = [x_1_i + self.F*x_diff_i for x_1_i, x_diff_i in zip (x_1, x_diff)]\n",
    "            donor_vec = self._ensure_bounds(donor_vec, bounds)\n",
    "            donor_population.append(donor_vec)\n",
    "\n",
    "        return donor_population\n",
    "\n",
    "    # recombine donor vectors according to crossover probability\n",
    "    def _recombination(self, population, donor_population):\n",
    "        trial_population = []\n",
    "        for k in range(self.n):\n",
    "            target_vec = population[k]\n",
    "            donor_vec = donor_population[k]\n",
    "            trial_vec = []\n",
    "            for p in range(len(self.parbounds)):\n",
    "                crossover = random.random()\n",
    "\n",
    "                # if random number is below set crossover probability do recombination\n",
    "                if crossover <= self.CR:\n",
    "                    trial_vec.append(donor_vec[p])\n",
    "                else:\n",
    "                    trial_vec.append(target_vec[p])\n",
    "            trial_population.append(trial_vec)\n",
    "        return trial_population\n",
    "\n",
    "    # select the best individuals from each generation\n",
    "    def _selection(self, population, trial_population):\n",
    "        # Calculate trial vectors and target vectors and select next generation\n",
    "\n",
    "        if self._generation == 0:\n",
    "            parsed_population = []\n",
    "            for target_vec in population:\n",
    "                parsed_target_vec = self._parse_back(target_vec)\n",
    "                parsed_population.append(parsed_target_vec)\n",
    "\n",
    "            parsed_population = self._parse_to_dict(parsed_population)\n",
    "            self._scores = self.objective_function(parsed_population)\n",
    "\n",
    "        parsed_trial_population = []\n",
    "        for index, trial_vec in enumerate(trial_population):\n",
    "            parsed_trial_vec = self._parse_back(trial_vec)\n",
    "            parsed_trial_population.append(parsed_trial_vec)\n",
    "\n",
    "        parsed_trial_population =  self._parse_to_dict(parsed_trial_population)\n",
    "        trial_population_scores = self.objective_function(parsed_trial_population)\n",
    "\n",
    "        for i in range(self.n):\n",
    "            trial_vec_score_i = trial_population_scores[i]\n",
    "            target_vec_score_i = self._scores[i]\n",
    "            if self.direction == 'max':\n",
    "                if trial_vec_score_i > target_vec_score_i:\n",
    "                    self._scores[index] = trial_vec_score_i\n",
    "                    population[index] = trial_vec\n",
    "            else:\n",
    "                if trial_vec_score_i < target_vec_score_i:\n",
    "                    self._scores[index] = trial_vec_score_i\n",
    "                    population[index] = trial_vec\n",
    "\n",
    "        self._generation += 1\n",
    "\n",
    "        return population\n",
    "\n",
    "    # parse the converted values back to original\n",
    "    def _parse_back(self, individual):\n",
    "        original_representation = []\n",
    "        for index, parameter in enumerate(individual):\n",
    "            if self.types[index] == self._types[2]:\n",
    "                original_representation.append(self.parbounds[index][parameter])\n",
    "            else:\n",
    "\n",
    "                original_representation.append(parameter)\n",
    "\n",
    "        return original_representation\n",
    "\n",
    "    # for parallelization purposes one can parse the population from a list to a  dictionary format\n",
    "    # User only has to add the parameters he wants to optimize to population_dict\n",
    "    def _parse_to_dict(self, population):\n",
    "        population_dict = {'learning_rate': [], 'dropout': []}\n",
    "        for indiv in population:\n",
    "            population_dict['learning_rate'].append(indiv[0])\n",
    "            population_dict['dropout'].append(indiv[1])\n",
    "\n",
    "        return population_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.40427283854272833], [0, 0.5756949048154342], [1, 0.6219462345112103], [0, 0.34593948993453627]]\n",
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1513605045578_0591/runId.0 for logfile and TensorBoard logdir\n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.0\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.0/learning_rate=0.1.dropout=0.404272838543.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.0/learning_rate=0.005.dropout=0.575694904815.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.0/learning_rate=0.1.dropout=0.621946234511.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.0/learning_rate=0.005.dropout=0.345939489935.log\n",
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1513605045578_0591/runId.1 for logfile and TensorBoard logdir\n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.1\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.1/learning_rate=0.005.dropout=0.322813825087.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.1/learning_rate=0.005.dropout=0.454776187919.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.1/learning_rate=0.1.dropout=0.260228456798.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.1/learning_rate=0.1.dropout=0.381147173695.log\n",
      "('Generation: ', 1, ' || ', 'Average score: ', 0.412988475, ', best score: ', 0.748047, 'best param: ', [0.1, 0.38114717369484025])\n",
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1513605045578_0591/runId.2 for logfile and TensorBoard logdir\n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.2\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.2/learning_rate=0.005.dropout=0.404272838543.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.2/learning_rate=0.1.dropout=0.524672368951.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.2/learning_rate=0.1.dropout=0.621946234511.log\n",
      "Path to log: \n",
      "hdfs:///Projects/MarcMusicClass/Logs/TensorFlow/application_1513605045578_0591/runId.2/learning_rate=0.1.dropout=0.536235201375.log\n",
      "('Generation: ', 2, ' || ', 'Average score: ', 0.401855725, ', best score: ', 0.703516, 'best param: ', [0.1, 0.5362352013748575])\n",
      "('Population: ', [[0.1, 0.40427283854272833], [0.005, 0.5756949048154342], [0.1, 0.6219462345112103], [0.1, 0.5362352013748575]])\n",
      "('Scores: ', [0.0980469, 0.697266, 0.108594, 0.703516])"
     ]
    }
   ],
   "source": [
    "diff_evo = DifferentialEvolution(execute_all,[(0.005, 0.1),(0.1, 0.9)], ['cat', 'float'], direction='max', maxiter=2,popsize=4)\n",
    "\n",
    "results = diff_evo.solve()\n",
    "\n",
    "print(\"Population: \", results[0])\n",
    "print(\"Scores: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
