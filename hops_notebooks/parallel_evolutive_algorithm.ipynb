{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Classification Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2803</td><td>application_1513605045578_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop26:8042/node/containerlogs/container_e28_1513605045578_0031_01_000001/MusicClassificationandGeneration__marccr00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "sep=\",;,;,;\"\n",
    "def wrapper(learning_rate, dropout):\n",
    "    from hops import hdfs\n",
    "    #TODO: add network here with all the import within necessary for the code within the function\n",
    "    \n",
    "    acc=0.87\n",
    "    hdfs.log(sep+str(acc)+sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Parallely executes and returns list of accuracies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_accuracy(v):\n",
    "    if sep in v[\"_c0\"]:\n",
    "        i = v[\"_c0\"].find(sep)\n",
    "        substr = v[\"_c0\"][i+len(sep):]\n",
    "        i = substr.find(sep)\n",
    "        return [substr[:i]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_all_accuracies(tensorboard_hdfs_logdir):\n",
    "    from hops import hdfs\n",
    "    print(tensorboard_hdfs_logdir)\n",
    "    results=[]\n",
    "    for i in range(4):\n",
    "        path_to_log=tensorboard_hdfs_logdir+\"/\"\n",
    "        for k in args_dict_grid.keys():\n",
    "            path_to_log+=k+\"=\"+str(args_dict_grid[k][i])+\".\"\n",
    "        path_to_log+=\"log\"\n",
    "        print(\"Path to log: \")\n",
    "        print(path_to_log)\n",
    "        raw = spark.read.csv(path_to_log, sep=\"\\n\")\n",
    "        #raw.show(10)\n",
    "        #raw.count()\n",
    "        r = raw.rdd.flatMap(lambda v: get_accuracy(v)).collect()\n",
    "        results.extend(r)\n",
    "\n",
    "    #print(results)\n",
    "    return [float(res) for res in results]\n",
    "\n",
    "def execute_all(args_dict):\n",
    "    from hops import tflauncher\n",
    "\n",
    "    tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict)\n",
    "    return get_all_accuracies(tensorboard_hdfs_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Evolutionary algorithm for hyperparameter optimization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 9, hadoop10, executor 2): java.io.IOException: Cannot run program \"/srv/hops/anaconda/anaconda/envs/MusicClassificationandGeneration/bin/python\": error=13, Permission denied\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: error=13, Permission denied\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Cannot run program \"/srv/hops/anaconda/anaconda/envs/MusicClassificationandGeneration/bin/python\": error=13, Permission denied\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "Caused by: java.io.IOException: error=13, Permission denied\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n",
      "\t... 14 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 33, in execute_all\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/MusicClassificationandGeneration/lib/python2.7/site-packages/hops/tflauncher.py\", line 56, in launch\n",
      "    nodeRDD.foreachPartition(_prepare_func(app_id, run_id, map_fun, args_dict))\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/rdd.py\", line 799, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/rdd.py\", line 1041, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/rdd.py\", line 1032, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/rdd.py\", line 906, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/hadoop/tmp/nm-local-dir/usercache/LVrzr5otANtf6K968175dJtcEOGvuLr95GQByRbaUOY/appcache/application_1513605045578_0031/container_e28_1513605045578_0031_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 9, hadoop10, executor 2): java.io.IOException: Cannot run program \"/srv/hops/anaconda/anaconda/envs/MusicClassificationandGeneration/bin/python\": error=13, Permission denied\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: error=13, Permission denied\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Cannot run program \"/srv/hops/anaconda/anaconda/envs/MusicClassificationandGeneration/bin/python\": error=13, Permission denied\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "Caused by: java.io.IOException: error=13, Permission denied\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n",
      "\t... 14 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.001,0.002], 'dropout': [0.45,0.05]}\n",
    "#TODO: add here evolutive algorithm\n",
    "accuracies = execute_all(args_dict)\n",
    "print accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
