{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fashion MNIST\n",
    "Requires to have the tfrecords (on github, hops_notebooks folder) in a folder in datasets called mnist (to be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sep=\",;,;,;\"\n",
    "# Function containing all the training, parameters are the ones to be optimized\n",
    "def wrapper_mnist(learning_rate, dropout,num_steps,batch_size, filters, filters_end, kernel, kernel_end):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    train_filenames = [hdfs.project_path() + \"mnist/train.tfrecords\"]\n",
    "    validation_filenames = [hdfs.project_path() + \"mnist/validation.tfrecords\"]\n",
    "\n",
    "    # Create the neural network\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "\n",
    "        # Define a scope for reusing the variables\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\n",
    "            # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "            # Reshape to match picture format [Height x Width x Channel]\n",
    "            # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "            x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv1 = tf.layers.conv2d(x, filters, kernel, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv2 = tf.layers.conv2d(conv1, filters_end, kernel_end, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "            # Flatten the data to a 1-D vector for the fully connected layer\n",
    "            fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            # Fully connected layer (in tf contrib folder for now)\n",
    "            fc1 = tf.layers.dense(fc1, 1024)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "            # Output layer, class prediction\n",
    "            out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode, params):\n",
    "\n",
    "        # Build the neural network\n",
    "        # Because Dropout have different behavior at training and prediction time, we\n",
    "        # need to create 2 distinct computation graphs that still share the same weights.\n",
    "        logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "        logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits_test, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, \n",
    "                                                                                labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "        image = tf.reshape(features[:10], [-1, 28, 28, 1])\n",
    "        tf.summary.image(\"image\", image)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "        return estim_specs\n",
    "\n",
    "\n",
    "    def data_input_fn(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "        def parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "            image.set_shape([28 * 28])\n",
    "\n",
    "            # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "            image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "            return image, label\n",
    "\n",
    "        def _input_fn():\n",
    "            # Import MNIST data\n",
    "            dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Map the parser over dataset, and batch results by up to batch_size\n",
    "            dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(buffer_size=128)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.repeat(repeat)\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "            features, labels = iterator.get_next()\n",
    "\n",
    "            return features, labels\n",
    "\n",
    "        return _input_fn\n",
    "\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=tensorboard.logdir(),\n",
    "        save_checkpoints_steps=10,\n",
    "        save_summary_steps=5,\n",
    "        log_step_count_steps=10)\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "          save_steps = run_config.save_summary_steps,\n",
    "          scaffold= tf.train.Scaffold(),\n",
    "          summary_op=tf.summary.merge_all())\n",
    "\n",
    "    mnist_estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "\n",
    "\n",
    "    train_input_fn = data_input_fn(train_filenames[0], batch_size=batch_size)\n",
    "    eval_input_fn = data_input_fn(validation_filenames[0], batch_size=batch_size)\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        mnist_estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=num_steps,\n",
    "        min_eval_frequency=5,\n",
    "        eval_hooks=[summary_hook]\n",
    "    )\n",
    "    \n",
    "    hdfs.log(\"Execution train and evaluate\")\n",
    "    experiment.train_and_evaluate()\n",
    "    hdfs.log(\"Finished execution train and evaluate\")\n",
    "\n",
    "    hdfs.log(\"Trying estimator evaluate: \")\n",
    "    accuracy_score = mnist_estimator.evaluate(input_fn=eval_input_fn, steps=num_steps)[\"accuracy\"]\n",
    "    hdfs.log(\"Done estimator evaluate: \")\n",
    "    \n",
    "    #Important, metric to optimize must be logged using hdfs and sep string to be easy to search.\n",
    "    hdfs.log(sep+str(accuracy_score)+sep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parallely executes and returns list of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(v):\n",
    "    if sep in v[\"_c0\"]:\n",
    "        i = v[\"_c0\"].find(sep)\n",
    "        substr = v[\"_c0\"][i+len(sep):]\n",
    "        i = substr.find(sep)\n",
    "        return [substr[:i]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_all_accuracies(tensorboard_hdfs_logdir, args_dict, number_params):\n",
    "    '''\n",
    "    Retrieves all accuracies from the parallel executions (each one is in a \n",
    "    different file, one per combination of wrapper function parameter)\n",
    "    '''\n",
    "    from hops import hdfs\n",
    "    print(tensorboard_hdfs_logdir)\n",
    "    hdfs.log(tensorboard_hdfs_logdir)\n",
    "    results=[]\n",
    "    \n",
    "    #Important, this must be ordered equally than _parse_to_dict function\n",
    "    population_dict = ['learning_rate', 'dropout',\n",
    "                          'num_steps','batch_size','filters','filters_end','kernel','kernel_end']\n",
    "    for i in range(number_params):\n",
    "        path_to_log=tensorboard_hdfs_logdir+\"/\"\n",
    "        for k in population_dict:\n",
    "            path_to_log+=k+\"=\"+str(args_dict[k][i])+\".\"\n",
    "        path_to_log+=\"log\"\n",
    "        print(\"Path to log: \")\n",
    "        hdfs.log(\"Path to log: \")\n",
    "        print(path_to_log)\n",
    "        hdfs.log(path_to_log)\n",
    "        raw = spark.read.csv(path_to_log, sep=\"\\n\")\n",
    "        \n",
    "        r = raw.rdd.flatMap(lambda v: get_accuracy(v)).collect()\n",
    "        results.extend(r)\n",
    "\n",
    "    #print(results)\n",
    "    return [float(res) for res in results]\n",
    "\n",
    "def execute_all(population_dict):\n",
    "    '''\n",
    "    Executes wrapper function with all values of population_dict parallely. \n",
    "    Returns a list of accuracies (or metric returned in the wrapper) in the \n",
    "    same order as in the population_dict.\n",
    "    '''\n",
    "    from hops import tflauncher\n",
    "    number_params=[len(v) for v in population_dict.values()][0]\n",
    "    tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper_mnist, population_dict)\n",
    "    return get_all_accuracies(tensorboard_hdfs_logdir, population_dict,number_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolutionary algorithm for hyperparameter optimization\n",
    "To run code just adapt the last fuction (parse_to_dict) to include the items you wanna optimize.\n",
    "Also adapt the bounds and types in the main section to reflect the parameters you wanna optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Differential evolution algorithm extended to allow for categorical and integer values for optimization of hyperparameter\n",
    "space in Neural Networks, including an option for parallelization.\n",
    "\n",
    "This algorithm will create a full population to be evaluated, unlike typical differential evolution where each\n",
    "individual get compared and selected sequentially. This allows the user to send a whole population of parameters\n",
    "to a cluster and run computations in parallel, after which each individual gets evaluated with their respective\n",
    "target or trial vector.\n",
    "\n",
    "User will have to define:\n",
    "- Objective function to be optimized\n",
    "- Bounds of each parameter (all possible values)\n",
    "- The Types of each parameter, in order to be able to evaluate categorical, integer or floating values.\n",
    "- Direction of the optimization, i.e. maximization or minimization\n",
    "- Number of iterations, i.e. the amount of generations the algorithm will run\n",
    "- The population size, rule of thumb is to take between 5-10 time the amount of parameters to optimize\n",
    "- Mutation faction between [0, 2)\n",
    "- Crossover between [0, 1], the higher the value the more mutated values will crossover\n",
    "'''\n",
    "\n",
    "import random\n",
    "from hops import hdfs\n",
    "\n",
    "class DifferentialEvolution:\n",
    "    _types = ['float', 'int', 'cat']\n",
    "    _generation = 0\n",
    "    _scores = []\n",
    "\n",
    "    def __init__(self, objective_function, parbounds, types, direction = 'max', maxiter=10, popsize=10, mutationfactor=0.5, crossover=0.7):\n",
    "        self.objective_function = objective_function\n",
    "        self.parbounds = parbounds\n",
    "        self.direction = direction\n",
    "        self.types = types\n",
    "        self.maxiter = maxiter\n",
    "        self.n = popsize\n",
    "        self.F = mutationfactor\n",
    "        self.CR = crossover\n",
    "\n",
    "        #self.m = -1 if maximize else 1\n",
    "\n",
    "    # run differential evolution algorithms\n",
    "    def solve(self):\n",
    "        # initialise generation based on individual representation\n",
    "        population, bounds = self._population_initialisation()\n",
    "        hdfs.log(str(population))\n",
    "        print(str(population))\n",
    "        for _ in range(self.maxiter):\n",
    "            donor_population = self._mutation(population, bounds)\n",
    "            trial_population = self._recombination(population, donor_population)\n",
    "            population = self._selection(population, trial_population)\n",
    "\n",
    "            new_gen_avg = sum(self._scores)/self.n\n",
    "\n",
    "            if self.direction == 'max':\n",
    "                new_gen_best = max(self._scores)\n",
    "            else:\n",
    "                new_gen_best = min(self._scores)\n",
    "            new_gen_best_param = self._parse_back(population[self._scores.index(new_gen_best)])\n",
    "\n",
    "            hdfs.log(\"Generation: \" + str(self._generation) + \" || \" + \"Average score: \" + str(new_gen_avg)+\n",
    "                  \", best score: \" + str(new_gen_best) + \"best param: \" + str(new_gen_best_param))\n",
    "\n",
    "            print(\"Generation: \" + str(self._generation) + \" || \" + \"Average score: \" + str(new_gen_avg)+\n",
    "                  \", best score: \" + str(new_gen_best) + \"best param: \" + str(new_gen_best_param))\n",
    "\n",
    "        parsed_back_population = []\n",
    "        for indiv in population:\n",
    "            parsed_back_population.append(self._parse_back(indiv))\n",
    "\n",
    "        return parsed_back_population, self._scores\n",
    "\n",
    "    # define bounds of each individual depending on type\n",
    "    def _individual_representation(self):\n",
    "        bounds = []\n",
    "\n",
    "        for index, item in enumerate(self.types):\n",
    "            b =()\n",
    "            # if categorical then take bounds from 0 to number of items\n",
    "            if item == self._types[2]:\n",
    "                b = (0, int(len(self.parbounds[index]) - 1))\n",
    "            # if float/int then take given bounds\n",
    "            else:\n",
    "                b = self.parbounds[index]\n",
    "            bounds.append(b)\n",
    "        return bounds\n",
    "\n",
    "    # initialise population\n",
    "    def _population_initialisation(self):\n",
    "        population = []\n",
    "        num_parameters = len(self.parbounds)\n",
    "        for i in range(self.n):\n",
    "            indiv = []\n",
    "            bounds = self._individual_representation()\n",
    "\n",
    "            for i in range(num_parameters):\n",
    "                indiv.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "            indiv = self._ensure_bounds(indiv, bounds)\n",
    "            population.append(indiv)\n",
    "        return population, bounds\n",
    "\n",
    "    # ensure that any mutated individual is within bounds\n",
    "    def _ensure_bounds(self, indiv, bounds):\n",
    "        indiv_correct = []\n",
    "\n",
    "        for i in range(len(indiv)):\n",
    "            par = indiv[i]\n",
    "\n",
    "            # check if param is within bounds\n",
    "            lowerbound = bounds[i][0]\n",
    "            upperbound = bounds[i][1]\n",
    "            if par < lowerbound:\n",
    "                par = lowerbound\n",
    "            elif par > upperbound:\n",
    "                par = upperbound\n",
    "\n",
    "            # check if param needs rounding\n",
    "            if self.types[i] != 'float':\n",
    "                par = int(round(par))\n",
    "            indiv_correct.append(par)\n",
    "        return indiv_correct\n",
    "\n",
    "    # create donor population based on mutation of three vectors\n",
    "    def _mutation(self, population, bounds):\n",
    "        donor_population = []\n",
    "        for i in range(self.n):\n",
    "\n",
    "            indiv_indices = list(range(0, self.n))\n",
    "            indiv_indices.remove(i)\n",
    "\n",
    "            candidates = random.sample(indiv_indices, 3)\n",
    "            x_1 = population[candidates[0]]\n",
    "            x_2 = population[candidates[1]]\n",
    "            x_3 = population[candidates[2]]\n",
    "\n",
    "            # substracting the second from the third candidate\n",
    "            x_diff = [x_2_i - x_3_i for x_2_i, x_3_i in zip(x_2, x_3)]\n",
    "            donor_vec = [x_1_i + self.F*x_diff_i for x_1_i, x_diff_i in zip (x_1, x_diff)]\n",
    "            donor_vec = self._ensure_bounds(donor_vec, bounds)\n",
    "            donor_population.append(donor_vec)\n",
    "\n",
    "        return donor_population\n",
    "\n",
    "    # recombine donor vectors according to crossover probability\n",
    "    def _recombination(self, population, donor_population):\n",
    "        trial_population = []\n",
    "        for k in range(self.n):\n",
    "            target_vec = population[k]\n",
    "            donor_vec = donor_population[k]\n",
    "            trial_vec = []\n",
    "            for p in range(len(self.parbounds)):\n",
    "                crossover = random.random()\n",
    "\n",
    "                # if random number is below set crossover probability do recombination\n",
    "                if crossover <= self.CR:\n",
    "                    trial_vec.append(donor_vec[p])\n",
    "                else:\n",
    "                    trial_vec.append(target_vec[p])\n",
    "            trial_population.append(trial_vec)\n",
    "        return trial_population\n",
    "\n",
    "    # select the best individuals from each generation\n",
    "    def _selection(self, population, trial_population):\n",
    "        # Calculate trial vectors and target vectors and select next generation\n",
    "\n",
    "        if self._generation == 0:\n",
    "            parsed_population = []\n",
    "            for target_vec in population:\n",
    "                parsed_target_vec = self._parse_back(target_vec)\n",
    "                parsed_population.append(parsed_target_vec)\n",
    "\n",
    "            parsed_population = self._parse_to_dict(parsed_population)\n",
    "            self._scores = self.objective_function(parsed_population)\n",
    "\n",
    "        parsed_trial_population = []\n",
    "        for index, trial_vec in enumerate(trial_population):\n",
    "            parsed_trial_vec = self._parse_back(trial_vec)\n",
    "            parsed_trial_population.append(parsed_trial_vec)\n",
    "\n",
    "        parsed_trial_population =  self._parse_to_dict(parsed_trial_population)\n",
    "        trial_population_scores = self.objective_function(parsed_trial_population)\n",
    "\n",
    "        hdfs.log('Pop scores: ' + str(self._scores))\n",
    "        print('Pop scores: ' + str(self._scores))\n",
    "        hdfs.log('Trial scores: ' + str(trial_population_scores))\n",
    "        print('Trial scores: ' + str(trial_population_scores))\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            trial_vec_score_i = trial_population_scores[i]\n",
    "            target_vec_score_i = self._scores[i]\n",
    "            if self.direction == 'max':\n",
    "                if trial_vec_score_i > target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "            else:\n",
    "                if trial_vec_score_i < target_vec_score_i:\n",
    "                    self._scores[i] = trial_vec_score_i\n",
    "                    population[i] = trial_population[i]\n",
    "\n",
    "        self._generation += 1\n",
    "\n",
    "        return population\n",
    "    # parse the converted values back to original\n",
    "    def _parse_back(self, individual):\n",
    "        original_representation = []\n",
    "        for index, parameter in enumerate(individual):\n",
    "            if self.types[index] == self._types[2]:\n",
    "                original_representation.append(self.parbounds[index][parameter])\n",
    "            else:\n",
    "\n",
    "                original_representation.append(parameter)\n",
    "\n",
    "        return original_representation\n",
    "\n",
    "    # for parallelization purposes one can parse the population from a list to a  dictionary format\n",
    "    # User only has to add the parameters he wants to optimize to population_dict\n",
    "    def _parse_to_dict(self, population):\n",
    "        population_dict = {'learning_rate':[], 'dropout':[],\n",
    "                          'num_steps':[],'batch_size':[],'filters':[],'filters_end':[],'kernel':[],'kernel_end':[]}\n",
    "        for indiv in population:\n",
    "            population_dict['learning_rate'].append(indiv[0])\n",
    "            population_dict['dropout'].append(indiv[1])\n",
    "            population_dict['num_steps'].append(indiv[2])\n",
    "            population_dict['batch_size'].append(indiv[3])\n",
    "            population_dict['filters'].append(indiv[4])\n",
    "            population_dict['filters_end'].append(indiv[5])\n",
    "            population_dict['kernel'].append(indiv[6])\n",
    "            population_dict['kernel_end'].append(indiv[7])\n",
    "            \n",
    "            \n",
    "        return population_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Observe that, for some combinations of parameters the network might exceed default RAM allocation on Hops\n",
    "\n",
    "#The parameters can be float, int or cat (categorical, tuple of values), this parameters must be specified in \n",
    "#function _parse_to_dict\n",
    "diff_evo = DifferentialEvolution(execute_all,\n",
    "                 [(0.001,0.02),(0.5,0.9),(50,300),(100,200),(25, 45),(55, 75),(3,7),(1,4)], \n",
    "                 ['float','float','int','int','int', 'int','int','int'], \n",
    "                 direction='max', maxiter=10,popsize=30)\n",
    "\n",
    "results = diff_evo.solve()\n",
    "\n",
    "print(\"Population: \", results[0])\n",
    "print(\"Scores: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
